"""
æ–‡æœ¬å†…å®¹è¿‡æ»¤å™¨æ¨¡å— (æœ€ç»ˆæ¶æ„ç‰ˆ v4 - å®æ—¶å…³é”®è¯)
é›†æˆäº† GFWList é»‘åå•ã€TLD ç™½åå•ã€é™æ€é“¾æ¥æ£€æµ‹ã€å®æ—¶å…³é”®è¯é»‘åå•ã€
ä»¥åŠç”±å¤šä¸ª OpenCC å¼•æ“é©±åŠ¨çš„æœ€é«˜ç²¾åº¦ç¹ä½“ä¸­æ–‡è¿‡æ»¤ç³»ç»Ÿã€‚
"""
import re
from urllib.parse import urlparse
import os
import datetime
import opencc

# --- 1. åˆå§‹åŒ–ä¸é…ç½® ---
# OpenCC å¤šå¼•æ“
OPENCC_CONVERTERS = []
try:
    configs = ['t2s', 'tw2s', 'hk2s']
    for config in configs:
        OPENCC_CONVERTERS.append(opencc.OpenCC(config))
    OPENCC_AVAILABLE = True
    print(f"âœ“ OpenCC å¤šå¼•æ“ç¹ä½“æ£€æµ‹ç³»ç»Ÿ ({len(OPENCC_CONVERTERS)}ä¸ªå¼•æ“) åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    OPENCC_CONVERTERS = []
    OPENCC_AVAILABLE = False
    print(f"âš ï¸ è­¦å‘Š: OpenCC åŠ è½½å¤±è´¥ ({e})ã€‚")

# è·¯å¾„é…ç½®
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
GFWLIST_DIR = os.path.join(BASE_DIR, 'gfwlist')
BLACKLIST_FILE = os.path.join(GFWLIST_DIR, 'list.txt')
TEXT_BLACKLIST_FILE = os.path.join(GFWLIST_DIR, 'text_list.txt')
LOG_DIR = os.path.join(GFWLIST_DIR, 'logs')

# çŠ¶æ€å˜é‡ (ä»…ç”¨äºéœ€è¦ç¼“å­˜çš„å¤§æ–‡ä»¶)
BLACKLISTED_DOMAINS = set()
GFWLIST_LOADED = False
LOG_FILE_PATH = None

# TLD ç™½åå• å’Œ é™æ€æ–‡ä»¶åç¼€
ALLOWED_TLDS = {'.com', '.cn', '.org', '.com.cn', '.gov', '.gov.cn', '.net'}
STATIC_EXTENSIONS = {'.html', '.htm'}


# --- 2. å†…å®¹ä¸æ ¼å¼å®¡æŸ¥ ---
def contains_chinese(text):
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fa5]', text))

def contains_traditional_chinese(text):
    if not text or not OPENCC_AVAILABLE: return False
    for converter in OPENCC_CONVERTERS:
        if converter.convert(text) != text:
            return True
    return False

def is_static_url(url):
    try:
        path = urlparse(url).path
        return any(path.lower().endswith(ext) for ext in STATIC_EXTENSIONS)
    except Exception:
        return False

def contains_blacklisted_keyword(text):
    """
    åŠ¨æ€æ£€æŸ¥ï¼šæ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è¯»å– text_list.txtï¼Œç¡®ä¿è§„åˆ™å®æ—¶ç”Ÿæ•ˆã€‚
    """
    if not text:
        return False
    try:
        # æ¯æ¬¡éƒ½é‡æ–°æ‰“å¼€å¹¶è¯»å–æ–‡ä»¶ï¼Œç¡®ä¿å®æ—¶æ€§
        with open(TEXT_BLACKLIST_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                keyword = line.strip()
                # ç¡®ä¿å…³é”®è¯éç©ºä¸”ä¸æ˜¯æ³¨é‡Šï¼Œç„¶åæ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
                if keyword and not keyword.startswith('!') and keyword in text:
                    return True # åªè¦æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±ç«‹å³è¿”å›
        return False # éå†å®Œæ–‡ä»¶éƒ½æœªæ‰¾åˆ°
    except FileNotFoundError:
        # åªæœ‰åœ¨ç¬¬ä¸€æ¬¡æ‰¾ä¸åˆ°æ–‡ä»¶æ—¶æ‰“å°è­¦å‘Šï¼Œé¿å…åˆ·å±
        if not hasattr(contains_blacklisted_keyword, 'warned_not_found'):
            print(f"âš ï¸ è­¦å‘Š: å…³é”®è¯é»‘åå• {TEXT_BLACKLIST_FILE} æœªæ‰¾åˆ°ï¼Œè¯¥è¿‡æ»¤è§„åˆ™å°†è·³è¿‡ã€‚")
            contains_blacklisted_keyword.warned_not_found = True
        return False
    except Exception as e:
        # åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å‘ç”Ÿè¯»å–é”™è¯¯æ—¶æ‰“å°è­¦å‘Š
        if not hasattr(contains_blacklisted_keyword, 'warned_error'):
            print(f"âŒ è¯»å–å…³é”®è¯é»‘åå•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            contains_blacklisted_keyword.warned_error = True
        return False

# --- 3. é»‘åå•ä¸ç™½åå•åŠ è½½ä¸æ£€æŸ¥ ---
def is_tld_whitelisted(url):
    try:
        domain = urlparse(url).netloc
        if not domain: return False
        return any(domain.endswith(tld) for tld in ALLOWED_TLDS)
    except Exception:
        return False

def load_gfwlist_blacklist():
    global BLACKLISTED_DOMAINS, GFWLIST_LOADED
    if GFWLIST_LOADED: return
    print(f"ğŸ”Œ æ­£åœ¨åŠ è½½ GFWList åŸŸåé»‘åå•: {BLACKLIST_FILE}")
    try:
        with open(BLACKLIST_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('!') and not line.startswith('['):
                    domain = ''
                    if line.startswith('||'): domain = re.sub(r'[/^*].*$', '', line[2:])
                    elif line.startswith('|http'):
                        try: domain = urlparse(line[1:]).netloc
                        except Exception: continue
                    else: domain = re.sub(r'[/^*].*$', '', re.sub(r'^[.@]*', '', line))
                    if domain and '.' in domain: BLACKLISTED_DOMAINS.add(domain)
        GFWLIST_LOADED = True
        print(f"âœ“ GFWList åŠ è½½æˆåŠŸï¼Œå…± {len(BLACKLISTED_DOMAINS)} æ¡è§„åˆ™ã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Š: GFWList é»‘åå•æ–‡ä»¶ {BLACKLIST_FILE} æœªæ‰¾åˆ°ã€‚")
    except Exception as e:
        print(f"âŒ åŠ è½½ GFWList æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def is_domain_blacklisted(url):
    if not GFWLIST_LOADED: load_gfwlist_blacklist()
    if not url or not BLACKLISTED_DOMAINS: return None
    try:
        domain = urlparse(url).netloc
        if not domain: return None
        parts = domain.split('.')
        for i in range(len(parts)):
            sub_domain = '.'.join(parts[i:])
            if sub_domain in BLACKLISTED_DOMAINS:
                return sub_domain
        return None
    except Exception:
        return None

# --- 4. æ—¥å¿— ---
def _get_log_file_path():
    global LOG_FILE_PATH
    if LOG_FILE_PATH: return LOG_FILE_PATH
    os.makedirs(LOG_DIR, exist_ok=True)
    date_str = datetime.datetime.now().strftime('%Y_%m_%d')
    sequence = 1
    while True:
        filename = f"{date_str}_{sequence:02d}_gfw_logs.txt"
        path = os.path.join(LOG_DIR, filename)
        if not os.path.exists(path):
            LOG_FILE_PATH = path
            return path
        sequence += 1

def log_filtered_event(url, reason, detail):
    try:
        log_path = _get_log_file_path()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] Blocked URL: {url} | Reason: {reason} | Detail: {detail}\n"
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(log_message)
    except Exception as e:
        print(f"âŒ å†™å…¥è¿‡æ»¤æ—¥å¿—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
