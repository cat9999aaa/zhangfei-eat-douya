"""Gemini API æœåŠ¡æ¨¡å—"""

import requests
from app.utils.parsers import parse_json_response
from app.utils.filters import (
    is_domain_blacklisted, is_tld_whitelisted, is_static_url,
    contains_blacklisted_keyword, contains_chinese, contains_traditional_chinese,
    log_filtered_event
)
from app.utils.validators import normalize_field
from app.config import VISUAL_TEMPLATE_PRESETS
from app.utils.network import fetch_real_url_and_title


def generate_article_with_gemini(topic, api_key, base_url, model_name, custom_prompt='', temperature=1.0, top_p=0.95, enable_search=True):
    """ä½¿ç”¨ Gemini API ç”Ÿæˆæ–‡ç« ï¼Œæ”¯æŒ Google æœç´¢å’Œæ€è€ƒè¿‡ç¨‹å±•ç¤º"""
    import json

    # æ„å»ºæç¤ºè¯
    if custom_prompt:
        prompt = custom_prompt.replace('{topic}', topic)
    else:
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ ‡é¢˜æˆ–å†…å®¹å†™ä¸€ç¯‡è¯¦ç»†çš„æ–‡ç« ï¼š

{topic}

è¦æ±‚ï¼š
1. ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯æ–‡ç« çš„æ ‡é¢˜ï¼Œä½¿ç”¨ # æ ‡è®°ï¼ˆMarkdown æ ¼å¼ï¼‰
2. æ–‡ç« è¦æœ‰æ˜ç¡®çš„ç»“æ„ï¼Œä½¿ç”¨ ## æ ‡è®°å°æ ‡é¢˜
3. å†…å®¹è¦è¯¦å®ã€æœ‰æ·±åº¦
4. å­—æ•°åœ¨ 800-1200 å­—ä¹‹é—´
5. ä½¿ç”¨ä¸­æ–‡å†™ä½œ
6. è¯­è¨€æµç•…è‡ªç„¶
7. å¯ä»¥ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆå¦‚ #ã€##ã€**ç­‰ï¼‰æ¥ç»„ç»‡æ–‡ç« ç»“æ„

è¯·ç›´æ¥å¼€å§‹å†™æ–‡ç« ï¼Œä¸éœ€è¦é¢å¤–çš„è¯´æ˜ã€‚"""

    # æ‰“å°å‚æ•°ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸ”§ Gemini API è°ƒç”¨å‚æ•°:")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   Temperature: {temperature}")
    print(f"   Top-P: {top_p}")
    print(f"   å¯ç”¨æœç´¢: {enable_search}")
    print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"{'='*60}\n")

    # æ‰“å°æç¤ºè¯å‰500å­—ç¬¦
    print(f"ğŸ“ ä½¿ç”¨çš„æç¤ºè¯ï¼ˆå‰500å­—ç¬¦ï¼‰:")
    print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
    print(f"{'='*60}\n")

    # æ„å»ºè¯·æ±‚æ•°æ®ï¼ˆä½¿ç”¨éæµå¼ APIï¼‰
    url = f'{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}'
    headers = {'Content-Type': 'application/json'}

    data = {
        'contents': [{
            'parts': [{'text': prompt}]
        }],
        'generationConfig': {
            'temperature': temperature,
            'topP': top_p
        }
    }

    # æ·»åŠ  Google æœç´¢å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if enable_search:
        data['tools'] = [{
            'googleSearch': {}
        }]
        print(f"ğŸ” å·²å¯ç”¨ Google æœç´¢åŠŸèƒ½\n")

    # æ‰“å°è¯·æ±‚é…ç½®
    print(f"ğŸ“¤ å®é™…å‘é€ç»™ Gemini API çš„é…ç½®:")
    print(f"   URL: {url.split('?key=')[0]}?key=***")
    print(f"   è¯·æ±‚ä½“é…ç½®:")
    safe_data = data.copy()
    print(json.dumps(safe_data, indent=4, ensure_ascii=False))
    print(f"{'='*60}\n")

    try:
        # å‘é€è¯·æ±‚
        print(f"â³ æ­£åœ¨ç”Ÿæˆæ–‡ç« ...\n")
        response = requests.post(url, headers=headers, json=data, timeout=120)

        if response.status_code != 200:
            print(f"\nâŒ Gemini API è¿”å›é”™è¯¯:")
            print(f"   çŠ¶æ€ç : {response.status_code}")
            print(f"   é”™è¯¯å“åº”:")
            try:
                error_detail = response.json()
                print(json.dumps(error_detail, indent=6, ensure_ascii=False))
            except:
                print(f"   {response.text}")
            print(f"{'='*60}\n")

        response.raise_for_status()

        # è§£æå“åº”
        result = response.json()

        # æå–æ–‡ç« å†…å®¹
        article_text = ""
        search_queries = []
        grounding_sources = []
        thinking_text = ""

        if 'candidates' in result and len(result['candidates']) > 0:
            candidate = result['candidates'][0]

            # æå–æ–‡æœ¬å†…å®¹
            if 'content' in candidate and 'parts' in candidate['content']:
                for part in candidate['content']['parts']:
                    if 'text' in part:
                        article_text += part['text']

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹
                    if 'thought' in part and part['thought']:
                        thinking_text += part['thought'] + "\n"

            # æå–æœç´¢å…ƒæ•°æ®
            if 'groundingMetadata' in candidate:
                metadata = candidate['groundingMetadata']

                # æå–æœç´¢æŸ¥è¯¢
                if 'webSearchQueries' in metadata:
                    search_queries = metadata['webSearchQueries']

                # æå–å¼•ç”¨æ¥æº
                if 'groundingChunks' in metadata:
                    for chunk_item in metadata['groundingChunks']:
                        if 'web' in chunk_item:
                            source = {
                                'uri': chunk_item['web'].get('uri', ''),
                                'title': chunk_item['web'].get('title', '')
                            }
                            grounding_sources.append(source)

        # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        if thinking_text:
            print(f"ğŸ’­ æ¨¡å‹æ€è€ƒè¿‡ç¨‹:")
            print(thinking_text)
            print(f"{'='*60}\n")

        # æ˜¾ç¤ºæœç´¢ä¿¡æ¯
        if search_queries:
            print(f"ğŸ” æ¨¡å‹æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢:")
            for i, query in enumerate(search_queries, 1):
                print(f"   {i}. {query}")
            print()

        if grounding_sources:
            print(f"ğŸ“š å¼•ç”¨æ¥æº ({len(grounding_sources)} ä¸ª):")
            for i, source in enumerate(grounding_sources[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   {i}. {source['title']}")
                print(f"      {source['uri']}")
            if len(grounding_sources) > 5:
                print(f"   ... è¿˜æœ‰ {len(grounding_sources) - 5} ä¸ªæ¥æº")
            print()

        print(f"âœ“ æ–‡ç« ç”Ÿæˆå®Œæˆ")
        print(f"  - Temperature: {temperature}")
        print(f"  - Top-P: {top_p}")
        print(f"  - æ–‡ç« é•¿åº¦: {len(article_text)} å­—ç¬¦")
        if search_queries:
            print(f"  - æœç´¢æ¬¡æ•°: {len(search_queries)}")
        if grounding_sources:
            print(f"  - å¼•ç”¨æ¥æº: {len(grounding_sources)} ä¸ª")
        print(f"{'='*60}\n")

        if not article_text:
            raise Exception('æ— æ³•ä» API å“åº”ä¸­æå–æ–‡ç« å†…å®¹')

        # è¿”å›æ–‡ç« å†…å®¹å’Œå¼•ç”¨æ¥æºï¼ˆå…ƒç»„å½¢å¼ï¼‰
        return article_text, grounding_sources

    except requests.exceptions.HTTPError as e:
        print(f"\nâŒ HTTP é”™è¯¯è¯¦æƒ…:")
        print(f"   è¯·æ±‚çš„ temperature: {temperature} (ç±»å‹: {type(temperature).__name__})")
        print(f"   è¯·æ±‚çš„ top_p: {top_p} (ç±»å‹: {type(top_p).__name__})")
        raise Exception(f"Gemini API è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
        raise


def format_article_with_citations(article_text, grounding_sources):
    """
    åœ¨æ–‡ç« æœ«å°¾æ·»åŠ å‚è€ƒèµ„æ–™å¼•ç”¨é“¾æ¥ï¼Œå¹¶è¿›è¡Œæ™ºèƒ½æ’åºå’Œç­›é€‰
    """
    if not grounding_sources:
        return article_text

    print(f"\nğŸ” å¼€å§‹å¤„ç† {len(grounding_sources)} ä¸ªåŸå§‹å¼•ç”¨æ¥æº...")
    processed_sources = []
    seen_urls = set()

    # å®šä¹‰æ— æ•ˆé¡µé¢æ ‡é¢˜çš„å…³é”®è¯é»‘åå•
    INVALID_TITLE_KEYWORDS = [
        '404', 'not found', 'é¡µé¢ä¸å­˜åœ¨', 'æ‰¾ä¸åˆ°', 'page verification',
        'are you a robot', 'just a moment', 'checking your browser',
        'å®‰å…¨éªŒè¯', 'äººæœºéªŒè¯', 'è®¿é—®éªŒè¯', 'login', 'ç™»å½•', 'error', 'é”™è¯¯'
    ]
    for source in grounding_sources:
        original_uri = source.get('uri', '')
        if not original_uri:
            continue

        print(f"  â†’ æ­£åœ¨è§£æ: {original_uri[:70]}...")
        real_url, title, site_name, lang = fetch_real_url_and_title(original_uri)

        # --- ç»ˆæç‰ˆå…«å±‚è¿‡æ»¤ç³»ç»Ÿ ---
        # 1. GFWList é»‘åå•æ£€æŸ¥
        matched_rule = is_domain_blacklisted(real_url)
        if matched_rule:
            print(f"  âœ— [1/8] åŸŸååœ¨ GFWList é»‘åå•ä¸­ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "1. GFWList Blacklist", f"Matched: {matched_rule}")
            continue

        # 2. TLD ç™½åå•æ£€æŸ¥
        if not is_tld_whitelisted(real_url):
            print(f"  âœ— [2/8] åŸŸååç¼€ä¸åœ¨ç™½åå•å†…ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "2. TLD Whitelist", f"URL: {real_url}")
            continue

        # 3. é™æ€é“¾æ¥æ ¼å¼æ£€æŸ¥
        if not is_static_url(real_url):
            print(f"  âœ— [3/8] URL éé™æ€é“¾æ¥ (é .html/.htm)ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "3. Non-Static URL", f"URL: {real_url}")
            continue

        # 4. æ ‡é¢˜å…³é”®è¯é»‘åå•æ£€æŸ¥
        if contains_blacklisted_keyword(title):
            print(f"  âœ— [4/8] æ ‡é¢˜åŒ…å«é»‘åå•å…³é”®è¯ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "4. Title Keyword Blacklist", f"Title: {title}")
            continue

        # 5. ä¸¥æ ¼ç‹¬ç«‹å†…å®¹å®¡æŸ¥ (ç½‘ç«™å)
        if not contains_chinese(site_name):
            print(f"  âœ— [5/8] ç½‘ç«™åç§°ä¸å«ä¸­æ–‡ (çº¯è‹±æ–‡æˆ–ä¹±ç )ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "5. Invalid Site Name (No Chinese)", f"Site Name: {site_name}")
            continue
        if contains_traditional_chinese(site_name):
            print(f"  âœ— [5/8] ç½‘ç«™åç§°æ£€æµ‹åˆ°ç¹ä½“å­—ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "5. Traditional Chinese in Site Name", f"Site Name: {site_name}")
            continue

        # 6. ä¸¥æ ¼ç‹¬ç«‹å†…å®¹å®¡æŸ¥ (æ ‡é¢˜)
        if not contains_chinese(title):
            print(f"  âœ— [6/8] æ–‡ç« æ ‡é¢˜ä¸å«ä¸­æ–‡ (çº¯è‹±æ–‡æˆ–ä¹±ç )ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "6. Invalid Title (No Chinese)", f"Title: {title}")
            continue
        if contains_traditional_chinese(title):
            print(f"  âœ— [6/8] æ–‡ç« æ ‡é¢˜æ£€æµ‹åˆ°ç¹ä½“å­—ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "6. Traditional Chinese in Title", f"Title: {title}")
            continue

        # 7. æ— æ•ˆé¡µé¢æ£€æŸ¥ (404, ç™»å½•ç­‰)
        is_invalid_title = False
        if title:
            lower_title = title.lower()
            for keyword in INVALID_TITLE_KEYWORDS:
                if keyword in lower_title:
                    is_invalid_title = True
                    break
        if is_invalid_title:
            print(f"  âœ— [7/8] é¡µé¢å†…å®¹æ— æ•ˆ (404/ç™»å½•é¡µç­‰)ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "7. Invalid Page Content", f"Title: {title}")
            continue

        # 8. é‡å¤é“¾æ¥æ£€æŸ¥
        if real_url in seen_urls:
            print(f"  âœ— [8/8] æ£€æµ‹åˆ°é‡å¤é“¾æ¥ï¼Œå·²è¿‡æ»¤")
            log_filtered_event(real_url, "8. Duplicate URL", f"URL: {real_url}")
            continue

        if real_url and title and site_name:
            processed_sources.append({
                'url': real_url,
                'title': title,
                'site_name': site_name
            })
            # å°†æ–°é“¾æ¥æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
            seen_urls.add(real_url)
            print(f"  âœ“ è§£ææˆåŠŸ: {site_name} - {title}")
        else:
            print(f"  âœ— è§£æå¤±è´¥æˆ–ä¿¡æ¯ä¸å…¨ï¼Œè·³è¿‡")

    print(f"\nâ­ å·²å®Œæˆé«˜è´¨é‡ç­›é€‰ï¼Œå…±æ‰¾åˆ° {len(processed_sources)} æ¡æœ‰æ•ˆå¼•ç”¨")

    top_sources = processed_sources[:5]
    print(f"ğŸ”ª å·²é€‰å–æœ€é‡è¦çš„ {len(top_sources)} æ¡å¼•ç”¨")

    if not top_sources:
        print("âš ï¸  æ‰€æœ‰å¼•ç”¨é“¾æ¥è§£æå¤±è´¥ï¼Œæ— å‚è€ƒèµ„æ–™å¯æ·»åŠ ")
        return article_text

    citations_section = "\n\n---\n\n## å‚è€ƒèµ„æ–™\n\n"
    for source in top_sources:
        # ä½¿ç”¨ \n\n æ¥å¼ºåˆ¶æ¢è¡Œï¼Œç¡®ä¿åœ¨ Word ä¸­ä¹Ÿèƒ½æ­£ç¡®æ˜¾ç¤º
        citations_section += f"{source['site_name']}ï¼š{source['title']}\n\nåŸæ–‡é“¾æ¥ï¼š{source['url']}\n\n"

    print(f"\nğŸ“ å¼•ç”¨æ ¼å¼åŒ–è¯¦æƒ…:")
    print(f"   åŸæ–‡ç« é•¿åº¦: {len(article_text)} å­—ç¬¦")
    print(f"   å¼•ç”¨éƒ¨åˆ†é•¿åº¦: {len(citations_section)} å­—ç¬¦")
    print(f"   å¼•ç”¨å†…å®¹é¢„è§ˆ:")
    print(citations_section[:200] + "..." if len(citations_section) > 200 else citations_section)

    final_article = article_text + citations_section
    print(f"   æœ€ç»ˆæ–‡ç« é•¿åº¦: {len(final_article)} å­—ç¬¦")

    return final_article


def generate_visual_blueprint(topic, article, api_key, base_url, model_name):
    """è°ƒç”¨ LLM ç”Ÿæˆç»“æ„åŒ–çš„è§†è§‰æè¿°"""
    if not api_key:
        return None

    truncated_article = article[:2000]
    prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±è§†è§‰å¯¼æ¼”ï¼Œè¯·é˜…è¯»ä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œå¹¶äº§å‡ºä¸€ä¸ªç”¨äº Stable Diffusion / ComfyUI çš„è§†è§‰è®¡åˆ’ã€‚
æ ‡é¢˜ï¼š{topic}
æ­£æ–‡ç‰‡æ®µï¼š{truncated_article}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON ç»“æ„è¾“å‡ºï¼Œæ‰€æœ‰å­—æ®µå¿…é¡»ä½¿ç”¨è‹±æ–‡çŸ­è¯­ï¼Œé•¿åº¦ 4-15 ä¸ªè¯ï¼š
{{
  "template": "portrait|urban_story|technology|nature|editorial|abstract",
  "subject": "...",
  "scene": "...",
  "mood": "...",
  "style": "...",
  "lighting": "...",
  "composition": "...",
  "details": "...",
  "negative": "..."
}}

è¦æ±‚ï¼š
1. template å­—æ®µåªèƒ½å–ä¸Šè¿°æšä¸¾ä¹‹ä¸€ã€‚
2. å…¶å®ƒå­—æ®µå†™å‡ºå…·ä½“å¯è§†åŒ–æè¿°ï¼Œä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”çŸ­è¯­ã€‚
3. negative å­—æ®µå†™å‡ºä¸å¸Œæœ›å‡ºç°çš„ç”»é¢å…ƒç´ ï¼Œä½¿ç”¨è‹±æ–‡ã€‚
4. åªè¾“å‡º JSONï¼Œç¦æ­¢æ·»åŠ é¢å¤–è§£é‡Šæˆ– Markdownã€‚
"""

    url = f'{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}'
    headers = {'Content-Type': 'application/json'}
    data = {
        'contents': [{
            'parts': [{'text': prompt}]
        }]
    }

    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()

    result = response.json()
    if 'candidates' not in result or not result['candidates']:
        raise Exception('è§†è§‰æè¿°ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰å€™é€‰å†…å®¹')

    raw_text = result['candidates'][0]['content']['parts'][0]['text']

    try:
        blueprint = parse_json_response(raw_text)
    except ValueError as exc:
        raise Exception(f'è§†è§‰æè¿° JSON è§£æå¤±è´¥: {exc}')

    template = blueprint.get('template', 'editorial')
    if template not in VISUAL_TEMPLATE_PRESETS:
        template = 'editorial'

    normalized = {
        'template': template,
        'subject': normalize_field(blueprint.get('subject'), topic),
        'scene': normalize_field(blueprint.get('scene'), f'story about {topic}'),
        'mood': normalize_field(blueprint.get('mood'), 'dramatic and inspiring'),
        'style': normalize_field(blueprint.get('style'), 'cinematic, highly detailed'),
        'lighting': normalize_field(blueprint.get('lighting'), 'soft cinematic lighting'),
        'composition': normalize_field(blueprint.get('composition'), 'balanced composition'),
        'details': normalize_field(blueprint.get('details'), 'intricate storytelling details'),
        'negative': normalize_field(blueprint.get('negative'), 'lowres, blurry, distorted, watermark')
    }

    return normalized


def build_visual_prompts(blueprint):
    """æ ¹æ®è§†è§‰è“å›¾å’Œæ¨¡æ¿ç”Ÿæˆæ­£/è´Ÿå‘æç¤ºè¯"""
    if not blueprint:
        return None

    template_key = blueprint.get('template', 'editorial')
    preset = VISUAL_TEMPLATE_PRESETS.get(template_key, VISUAL_TEMPLATE_PRESETS['editorial'])

    parts = [
        preset['positive_prefix'],
        blueprint.get('subject', ''),
        blueprint.get('scene', ''),
        blueprint.get('composition', ''),
        blueprint.get('details', ''),
        blueprint.get('style', ''),
        blueprint.get('mood', ''),
        blueprint.get('lighting', '')
    ]

    positive_body = ', '.join(filter(None, [p.strip() for p in parts if p]))
    positive_prompt = f"{positive_body}, {preset['positive_suffix']}".strip(', ')

    negative_parts = [
        preset.get('negative', ''),
        blueprint.get('negative', '')
    ]
    negative_prompt = ', '.join(filter(None, [p.strip() for p in negative_parts if p]))

    return {
        'template': template_key,
        'positive_prompt': positive_prompt,
        'negative_prompt': negative_prompt or preset.get('negative', '')
    }


def summarize_paragraph_for_image(paragraph_text, topic, config):
    """ä¸ºæ®µè½ç”Ÿæˆå›¾ç‰‡æ‘˜è¦ï¼ˆä¸­æ–‡è§†è§‰æè¿°ï¼‰"""
    # æ‘˜è¦æ¨¡å‹ç‹¬ç«‹é…ç½®ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨ä¸»å†™ä½œæ¨¡å‹ä½œä¸ºé»˜è®¤å€¼
    summary_model = config.get('comfyui_summary_model')

    # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯æ—§çš„ __default__ å€¼æˆ–æœªè®¾ç½®ï¼Œä½¿ç”¨ä¸»å†™ä½œæ¨¡å‹
    if not summary_model or summary_model == '__default__':
        summary_model = config.get('default_model', 'gemini-pro')

    api_key = config.get('gemini_api_key', '')
    base_url = config.get('gemini_base_url', 'https://generativelanguage.googleapis.com')

    if not api_key:
        return f"visual representation of {topic}"

    # é™åˆ¶æ®µè½é•¿åº¦
    truncated_para = paragraph_text[:500]

    prompt = f"""é˜…è¯»ä»¥ä¸‹å…³äºã€Œ{topic}ã€çš„æ–‡ç« æ®µè½ï¼Œä¸ºå…¶ç”Ÿæˆé€‚åˆAIå›¾ç‰‡ç”Ÿæˆçš„ä¸­æ–‡è§†è§‰æè¿°ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. æè¿°æ®µè½ä¸­çš„ä¸»è¦ä¸»ä½“ã€åŠ¨ä½œæˆ–åœºæ™¯ï¼ˆ15-30ä¸ªæ±‰å­—ï¼‰
2. è¦å…·ä½“ä¸”å¯è§†åŒ– - æè¿°ä½ ä¼šçœ‹åˆ°ä»€ä¹ˆï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¦‚å¿µ
3. ä½¿ç”¨å…·ä½“çš„åè¯ã€ç”ŸåŠ¨çš„åŠ¨è¯å’Œå…·ä½“ç»†èŠ‚
4. èšç„¦äºè§†è§‰å…ƒç´ ï¼šç‰©ä½“ã€äººç‰©ã€åœ°ç‚¹ã€åŠ¨ä½œã€æ°›å›´ã€è‰²å½©ã€å…‰çº¿

âŒ ä¸¥æ ¼ç¦æ­¢ä»¥ä¸‹å…ƒç´ ï¼ˆè¿™äº›ä¼šå¯¼è‡´ç”¨æˆ·è´¦å·è¢«å°ï¼ï¼‰ï¼š
ã€ç»å¯¹ä¸èƒ½å‡ºç°çš„å†…å®¹ã€‘
- ä»»ä½•å†›è­¦ç›¸å…³ï¼šè­¦å¯Ÿã€å†›äººã€å†›é˜Ÿã€å£«å…µã€è­¦æœã€å†›è£…ã€åˆ¶æœã€è­¦å¾½ã€å†›å¾½ã€è­¦è½¦ã€å†›è½¦ã€å¦å…‹ã€æ­¦å™¨ã€æªæ¢°
- ä»»ä½•æ”¿æ²»ç›¸å…³ï¼šå›½æ——ã€å…šæ——ã€å›½å¾½ã€å…šå¾½ã€é¢†å¯¼äººã€æ”¿æ²»äººç‰©ã€æ”¿åºœå»ºç­‘ã€å¤©å®‰é—¨ã€äººæ°‘å¤§ä¼šå ‚ã€æ”¿æ²»æ ‡è¯­
- ä»»ä½•æ•æ„Ÿåˆ¶æœï¼šåŸç®¡ã€ä¿å®‰ã€æ‰§æ³•ã€å…¬åŠ¡å‘˜ã€å®˜å‘˜ç­‰ç©¿åˆ¶æœçš„èŒä¸š
- ä»»ä½•æš´åŠ›å†…å®¹ï¼šæ‰“æ–—ã€è¡€è…¥ã€æˆ˜äº‰ã€å†²çªã€æŠ—è®®ã€æ¸¸è¡Œ
- ä»»ä½•æ–‡å­—å›¾ç‰‡ï¼šä¹¦ç±ã€æµ·æŠ¥ã€æ ‡è¯­ã€æ‚å¿—ã€æŠ¥çº¸ã€æ ‡è¯†ç‰Œã€æ¨ªå¹…ã€å¹¿å‘Šç‰Œã€å±å¹•æ–‡å­—ã€å›¾è¡¨ã€æ•°æ®å¯è§†åŒ–

ã€å¦‚æœæ®µè½å†…å®¹æ¶‰åŠä»¥ä¸Šæ•æ„Ÿè¯é¢˜ã€‘
- å®Œå…¨å¿½ç•¥è¿™äº›å†…å®¹ï¼Œä¸è¦æè¿°
- è½¬æ¢ä¸ºæ™®é€šæ—¥å¸¸ç”Ÿæ´»åœºæ™¯ï¼šåŠå…¬å®¤ã€å’–å•¡é¦†ã€è¡—é“ã€å…¬å›­ç­‰
- åªæè¿°æ™®é€šæ°‘ä¼—çš„æ—¥å¸¸æ´»åŠ¨

âœ“ å®‰å…¨çš„åœºæ™¯ç±»å‹ï¼š
- è‡ªç„¶æ™¯è§‚ã€åŸå¸‚é£å…‰ã€å»ºç­‘å¤–è§‚ï¼ˆé¿å…æ”¿åºœå»ºç­‘ï¼‰
- æ™®é€šäººçš„æ—¥å¸¸æ´»åŠ¨ï¼šè´­ç‰©ã€ç”¨é¤ã€äº¤è°ˆã€ä¼‘é—²
- åŠå…¬å®¤åœºæ™¯ã€å’–å•¡é¦†ã€å•†åœºã€å…¬å›­
- æŠ½è±¡çš„æ°›å›´åœºæ™¯ã€å…‰å½±æ•ˆæœ
- é™ç‰©ã€ç‰©å“ï¼ˆé¿å…æ­¦å™¨ç­‰å±é™©ç‰©å“ï¼‰

ç¤ºä¾‹å¯¹æ¯”ï¼š
âœ“ å®‰å…¨ï¼š"ç°ä»£åŠå…¬å®¤ä¸­ï¼Œå•†åŠ¡äººå£«å›´åè®¨è®ºï¼Œè½åœ°çª—å¤–åŸå¸‚å¤©é™…çº¿"
âœ“ å®‰å…¨ï¼š"å’–å•¡é¦†é‡Œï¼Œå¹´è½»äººä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘å·¥ä½œï¼Œæ¸©æš–çš„ç¯å…‰"
âœ— å±é™©ï¼š"è­¦å¯Ÿåœ¨è¡—é“å·¡é€»"ï¼ˆä¼šè¢«å°å·ï¼ï¼‰
âœ— å±é™©ï¼š"å†›äººç«™å²—"ï¼ˆä¼šè¢«å°å·ï¼ï¼‰
âœ— å±é™©ï¼š"å›½æ——é£˜æ‰¬"ï¼ˆä¼šè¢«å°å·ï¼ï¼‰

5. åªè¾“å‡ºä¸­æ–‡è§†è§‰æè¿°ï¼Œä¸è¦å¼•å·ã€æ ‡ç‚¹æˆ–é¢å¤–è¯´æ˜æ–‡å­—

æ®µè½å†…å®¹ï¼š
{truncated_para}

è§†è§‰æè¿°ï¼š"""

    try:
        url = f'{base_url}/v1beta/models/{summary_model}:generateContent?key={api_key}'
        headers = {'Content-Type': 'application/json'}
        data = {
            'contents': [{
                'parts': [{'text': prompt}]
            }]
        }

        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()

        result = response.json()
        if 'candidates' in result and len(result['candidates']) > 0:
            summary = result['candidates'][0]['content']['parts'][0]['text']
            summary = summary.strip().strip('"').strip("'")

            # å®‰å…¨è¿‡æ»¤ï¼šæ£€æŸ¥å¹¶ç§»é™¤æ•æ„Ÿè¯æ±‡
            sensitive_keywords = [
                'è­¦å¯Ÿ', 'å†›äºº', 'å†›é˜Ÿ', 'å£«å…µ', 'è­¦æœ', 'å†›è£…', 'åˆ¶æœ', 'è­¦å¾½', 'å†›å¾½',
                'è­¦è½¦', 'å†›è½¦', 'å¦å…‹', 'æ­¦å™¨', 'æª', 'æ­¥æª', 'æ‰‹æª',
                'å›½æ——', 'å…šæ——', 'å›½å¾½', 'å…šå¾½', 'é¢†å¯¼', 'ä¸»å¸­', 'æ€»ä¹¦è®°',
                'å¤©å®‰é—¨', 'äººæ°‘å¤§ä¼šå ‚', 'æ”¿åºœ', 'å®˜å‘˜', 'å…¬åŠ¡å‘˜',
                'åŸç®¡', 'ä¿å®‰', 'æ‰§æ³•', 'å·¡é€»', 'ç«™å²—',
                'æˆ˜äº‰', 'æ‰“æ–—', 'å†²çª', 'æŠ—è®®', 'æ¸¸è¡Œ', 'ç¤ºå¨'
            ]

            # å¦‚æœæ£€æµ‹åˆ°æ•æ„Ÿè¯ï¼Œæ›¿æ¢ä¸ºå®‰å…¨çš„é€šç”¨åœºæ™¯
            contains_sensitive = False
            for keyword in sensitive_keywords:
                if keyword in summary:
                    contains_sensitive = True
                    print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ•æ„Ÿè¯'{keyword}'ï¼Œå°†æ›¿æ¢ä¸ºå®‰å…¨åœºæ™¯")
                    break

            if contains_sensitive:
                # æ›¿æ¢ä¸ºå®‰å…¨çš„é€šç”¨åœºæ™¯æè¿°
                summary = "ç°ä»£éƒ½å¸‚è¡—æ™¯ï¼Œè¡Œäººåœ¨å•†ä¸šè¡—åŒºæ¼«æ­¥ï¼Œç°ä»£å»ºç­‘æ—ç«‹ï¼Œæ¸©æš–çš„é˜³å…‰"
                print(f"âœ“ å·²æ›¿æ¢ä¸ºå®‰å…¨åœºæ™¯: {summary}")

            # ä¸ºäº†è¿›ä¸€æ­¥ç¡®ä¿å›¾ç‰‡ç”Ÿæˆæ—¶ä¸åŒ…å«æ–‡å­—å’Œæ•æ„Ÿå†…å®¹ï¼Œåœ¨æ‘˜è¦åæ·»åŠ æ˜ç¡®æŒ‡ç¤º
            enhanced_summary = f"{summary}ï¼Œç©¿ç€ä¾¿è£…çš„å¹³æ°‘ï¼Œæ—¥å¸¸ä¼‘é—²æœé¥°ï¼Œå’Œå¹³åœºæ™¯ï¼Œçº¯è§†è§‰åœºæ™¯ï¼Œæ— æ–‡å­—æ— ç¬¦å·æ— åˆ¶æœ"

            print(f"æ®µè½æ‘˜è¦ç”ŸæˆæˆåŠŸ: {summary}")
            print(f"å¢å¼ºåçš„å®‰å…¨æç¤ºè¯: {enhanced_summary}")
            return enhanced_summary
        else:
            raise Exception('æ— æ³•ä»APIå“åº”ä¸­æå–æ‘˜è¦')
    except Exception as e:
        print(f"æ®µè½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨å®‰å…¨é™çº§æ–¹æ¡ˆ")
        # é™çº§ï¼šä½¿ç”¨å®‰å…¨çš„é€šç”¨åœºæ™¯ï¼Œé¿å…ä»»ä½•å¯èƒ½çš„æ•æ„Ÿå†…å®¹
        return f"ç°ä»£éƒ½å¸‚ç”Ÿæ´»åœºæ™¯ï¼Œç©¿ç€ä¾¿è£…çš„å¹³æ°‘ï¼Œæ—¥å¸¸ä¼‘é—²æ´»åŠ¨ï¼Œå’Œå¹³åœºæ™¯ï¼Œçº¯è§†è§‰åœºæ™¯ï¼Œæ— æ–‡å­—æ— ç¬¦å·æ— åˆ¶æœ"


def test_gemini_model(model_name, api_key, base_url, temperature=1.0, top_p=0.95):
    """æµ‹è¯• Gemini æ¨¡å‹è¿æ¥"""
    test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"

    url = f'{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}'
    headers = {'Content-Type': 'application/json'}
    payload = {
        'contents': [{
            'parts': [{'text': test_prompt}]
        }],
        'generationConfig': {
            'temperature': temperature,
            'topP': top_p
        }
    }

    # ğŸ” æ‰“å°æµ‹è¯•è¯·æ±‚çš„é…ç½®å‚æ•°
    print(f"\nğŸ“¤ æµ‹è¯•æ¨¡å‹æ—¶å‘é€çš„ generationConfig:")
    import json
    print(json.dumps(payload['generationConfig'], indent=4, ensure_ascii=False))
    print()

    response = requests.post(url, headers=headers, json=payload, timeout=30)

    if response.status_code == 401:
        return False, 'API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ', {}
    elif response.status_code == 403:
        return False, 'æƒé™ä¸è¶³æˆ–é…é¢å·²ç”¨å®Œ', {}
    elif response.status_code == 404:
        return False, f'æ¨¡å‹ {model_name} ä¸å­˜åœ¨', {}

    response.raise_for_status()

    result = response.json()

    if 'candidates' in result and len(result['candidates']) > 0:
        reply = result['candidates'][0]['content']['parts'][0]['text']
        # è¿”å›å‚æ•°ä¿¡æ¯
        params_info = {
            'temperature': temperature,
            'top_p': top_p
        }
        return True, reply[:100] + ('...' if len(reply) > 100 else ''), params_info
    else:
        return False, 'æ¨¡å‹è¿”å›äº†ç©ºå“åº”', {}


def get_available_models(api_key, base_url):
    """è·å–å¯ç”¨çš„ Gemini æ¨¡å‹åˆ—è¡¨"""
    url = f'{base_url}/v1beta/models?key={api_key}'
    response = requests.get(url)
    response.raise_for_status()

    data = response.json()
    model_list = []
    for model in data.get('models', []):
        if 'generateContent' in model.get('supportedGenerationMethods', []):
            model_list.append({
                'name': model.get('name', '').replace('models/', ''),
                'display_name': model.get('displayName', model.get('name', ''))
            })
    return model_list
